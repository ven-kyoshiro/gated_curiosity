{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Classic cart-pole system implemented by Rich Sutton et al.\n",
    "Copied from http://incompleteideas.net/sutton/book/code/pole.c\n",
    "permalink: https://perma.cc/C9ZM-652R\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import gym\n",
    "from gym import spaces, logger\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "\n",
    "class CartPoleEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The pendulum starts upright, and the goal is to prevent it from falling over by increasing and reducing the cart's velocity.\n",
    "    Source:\n",
    "        This environment corresponds to the version of the cart-pole problem described by Barto, Sutton, and Anderson\n",
    "    Observation: \n",
    "        Type: Box(4)\n",
    "        Num\tObservation                 Min         Max\n",
    "        0\tCart Position             -4.8            4.8\n",
    "        1\tCart Velocity             -Inf            Inf\n",
    "        2\tPole Angle                 -pi            +pi\n",
    "        3\tPole Velocity At Tip      -Inf            Inf\n",
    "        4\tWhisper                   Pole Angle is less than π±7°then randomly 0 ~ 1, else 0.\n",
    "        \n",
    "    Actions:\n",
    "        Type: Discrete(2)\n",
    "        Num\tAction\n",
    "        0\tPush cart to the left\n",
    "        1\tPush cart to the right\n",
    "        \n",
    "        Note: The amount the velocity is reduced or increased is not fixed as it depends on the angle the pole is pointing. This is because the center of gravity of the pole increases the amount of energy needed to move the cart underneath it\n",
    "    Reward:\n",
    "        Pole Angle is less than ±7°then + 1.0\n",
    "    Starting State:\n",
    "        All observations are assigned a uniform random value between ±0.05\n",
    "    Episode Termination:\n",
    "        Cart Position is more than ±2.4 (center of the cart reaches the edge of the display)\n",
    "        Episode length is greater than 200\n",
    "        Solved Requirements\n",
    "        Considered solved when the average reward is greater than or equal to 195.0 over 100 consecutive trials.\n",
    "    \"\"\"\n",
    "    \n",
    "    metadata = {\n",
    "        'render.modes': ['human', 'rgb_array'],\n",
    "        'video.frames_per_second' : 50\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = (self.masspole + self.masscart)\n",
    "        self.length = 0.5 # actually half the pole's length\n",
    "        self.polemass_length = (self.masspole * self.length)\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        self.kinematics_integrator = 'euler'\n",
    "\n",
    "        # Angle at which to fail the episode\n",
    "        self.theta_threshold_radians = 7 * 2 * math.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        # Angle limit set to 2 * theta_threshold_radians so failing observation is still within bounds\n",
    "        high = np.array([\n",
    "            self.x_threshold * 2,\n",
    "            np.finfo(np.float32).max,\n",
    "            self.theta_threshold_radians * 2,\n",
    "            np.finfo(np.float32).max,\n",
    "            1.])\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "\n",
    "        self.seed()\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
    "        state = self.state\n",
    "        x, x_dot, theta, theta_dot, whisper = state\n",
    "        force = self.force_mag if action==1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta* temp) / (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass))\n",
    "        xacc  = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "        if self.kinematics_integrator == 'euler':\n",
    "            x  = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else: # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x  = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "        theta = self.adj_theta(theta)\n",
    "        if theta > math.pi - self.theta_threshold_radians\\\n",
    "            or theta < -math.pi + self.theta_threshold_radians:\n",
    "            whisper = self.np_random.uniform(low=0.0, high=1.0, size=(1,))[0]\n",
    "        self.state = (x,x_dot,theta,theta_dot,whisper)\n",
    "        done =  x < -self.x_threshold \\\n",
    "                or x > self.x_threshold\n",
    "        done = bool(done)\n",
    "        \n",
    "\n",
    "        if theta > -self.theta_threshold_radians \\\n",
    "               and theta < self.theta_threshold_radians:\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            reward = 0.0\n",
    "\n",
    "        if not done:\n",
    "            pass\n",
    "            # reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            # reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                logger.warn(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n",
    "            self.steps_beyond_done += 1\n",
    "            # reward = 0.0\n",
    "\n",
    "        return np.array(self.state), reward, done, {}\n",
    "    \n",
    "    def adj_theta(self,th):\n",
    "        if th >= math.pi:\n",
    "            return -math.pi + (th - math.pi)\n",
    "        elif th < -math.pi:\n",
    "            return math.pi + (th + math.pi)\n",
    "        else:\n",
    "            return th\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.np_random.uniform(low=-0.01, high=0.01, size=(5,))\n",
    "        self.state[2] += math.pi # theta\n",
    "        self.state[2] = self.adj_theta(self.state[2])\n",
    "        self.state[4] = 0.\n",
    "        \n",
    "        \n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = self.x_threshold*2\n",
    "        scale = screen_width/world_width\n",
    "        carty = 100 # TOP OF CART\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * (2 * self.length)\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            l,r,t,b = -cartwidth/2, cartwidth/2, cartheight/2, -cartheight/2\n",
    "            axleoffset =cartheight/4.0\n",
    "            cart = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
    "            self.carttrans = rendering.Transform()\n",
    "            cart.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(cart)\n",
    "            l,r,t,b = -polewidth/2,polewidth/2,polelen-polewidth/2,-polewidth/2\n",
    "            pole = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
    "            pole.set_color(.8,.6,.4)\n",
    "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
    "            pole.add_attr(self.poletrans)\n",
    "            pole.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(pole)\n",
    "            self.axle = rendering.make_circle(polewidth/2)\n",
    "            self.axle.add_attr(self.poletrans)\n",
    "            self.axle.add_attr(self.carttrans)\n",
    "            self.axle.set_color(.5,.5,.8)\n",
    "            self.viewer.add_geom(self.axle)\n",
    "            self.track = rendering.Line((0,carty), (screen_width,carty))\n",
    "            self.track.set_color(0,0,0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            self._pole_geom = pole\n",
    "\n",
    "        if self.state is None: return None\n",
    "\n",
    "        # Edit the pole polygon vertex\n",
    "        pole = self._pole_geom\n",
    "        l,r,t,b = -polewidth/2,polewidth/2,polelen-polewidth/2,-polewidth/2\n",
    "        pole.v = [(l,b), (l,t), (r,t), (r,b)]\n",
    "\n",
    "        x = self.state\n",
    "        cartx = x[0]*scale+screen_width/2.0 # MIDDLE OF CART\n",
    "        self.carttrans.set_translation(cartx, carty)\n",
    "        self.poletrans.set_rotation(-x[2])\n",
    "\n",
    "        return self.viewer.render(return_rgb_array = mode=='rgb_array')\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CartPoleEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box(5,)\n",
      "action space: Discrete(2)\n",
      "initial observation: [-2.36905608e-03 -2.01788216e-03 -3.13267611e+00 -8.87320255e-03\n",
      "  0.00000000e+00]\n",
      "next observation: [-2.40941372e-03 -1.97266568e-01 -3.13285358e+00 -3.04356017e-01\n",
      "  3.68691161e-01]\n",
      "reward: 0.0\n",
      "done: False\n",
      "info: {}\n"
     ]
    }
   ],
   "source": [
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "obs = env.reset()\n",
    "# env.render()\n",
    "print('initial observation:', obs)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "obs, r, done, info = env.step(action)\n",
    "print('next observation:', obs)\n",
    "print('reward:', r)\n",
    "print('done:', done)\n",
    "print('info:', info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import chainerrl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFunction(chainer.Chain):\n",
    "\n",
    "    def __init__(self, obs_size, n_actions, n_hidden_channels=50):\n",
    "        super().__init__()\n",
    "        with self.init_scope():\n",
    "            self.l0 = L.Linear(obs_size, n_hidden_channels)\n",
    "            self.l1 = L.Linear(n_hidden_channels, n_hidden_channels)\n",
    "            self.l2 = L.Linear(n_hidden_channels, n_actions)\n",
    "\n",
    "    def __call__(self, x, test=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (ndarray or chainer.Variable): An observation\n",
    "            test (bool): a flag indicating whether it is in test mode\n",
    "        \"\"\"\n",
    "        h = F.tanh(self.l0(x))\n",
    "        h = F.tanh(self.l1(h))\n",
    "        return chainerrl.action_value.DiscreteActionValue(self.l2(h))\n",
    "\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "q_func = QFunction(obs_size, n_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "_q_func = chainerrl.q_functions.FCStateQFunctionWithDiscreteAction(\n",
    "    obs_size, n_actions,\n",
    "    n_hidden_layers=2, n_hidden_channels=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chainer.optimizers.adam.Adam at 0x7f2082f1f1d0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = chainer.optimizers.Adam(eps=1e-2)\n",
    "optimizer.setup(q_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the discount factor that discounts future rewards.\n",
    "gamma = 0.95\n",
    "\n",
    "# Use epsilon-greedy for exploration\n",
    "explorer = chainerrl.explorers.ConstantEpsilonGreedy(\n",
    "    epsilon=0.3, random_action_func=env.action_space.sample)\n",
    "\n",
    "# DQN uses Experience Replay.\n",
    "# Specify a replay buffer and its capacity.\n",
    "replay_buffer = chainerrl.replay_buffer.ReplayBuffer(capacity=10 ** 6)\n",
    "\n",
    "# Since observations from CartPole-v0 is numpy.float64 while\n",
    "# Chainer only accepts numpy.float32 by default, specify\n",
    "# a converter as a feature extractor function phi.\n",
    "phi = lambda x: x.astype(np.float32, copy=False)\n",
    "\n",
    "# Now create an agent that will interact with the environment.\n",
    "agent = chainerrl.agents.DoubleDQN(\n",
    "    q_func, optimizer, replay_buffer, gamma, explorer,\n",
    "    replay_start_size=500, update_interval=1,\n",
    "    target_update_interval=100, phi=phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 10 R: 0.0 statistics: [('average_q', 0.2793726498781115), ('average_loss', 0.012423035239966048)]\n",
      "episode: 20 R: 0.0 statistics: [('average_q', 0.37153260958445394), ('average_loss', 0.003455576131819604)]\n",
      "episode: 30 R: 0.0 statistics: [('average_q', 0.351040803621503), ('average_loss', 0.0026274855687934647)]\n",
      "episode: 40 R: 0.0 statistics: [('average_q', 0.34285769434519336), ('average_loss', 0.002296674191597305)]\n",
      "episode: 50 R: 0.0 statistics: [('average_q', 0.32577654475920154), ('average_loss', 0.0020616935978116217)]\n",
      "episode: 60 R: 0.0 statistics: [('average_q', 0.331422318883958), ('average_loss', 0.002838981506926369)]\n",
      "episode: 70 R: 0.0 statistics: [('average_q', 0.34651101084307717), ('average_loss', 0.0030478440450070914)]\n",
      "episode: 80 R: 0.0 statistics: [('average_q', 0.3193398678729252), ('average_loss', 0.0013720612593365897)]\n",
      "episode: 90 R: 0.0 statistics: [('average_q', 0.26515769866264965), ('average_loss', 0.0013151821088973651)]\n",
      "episode: 100 R: 0.0 statistics: [('average_q', 0.2373718811870791), ('average_loss', 0.0013696285857653426)]\n",
      "episode: 110 R: 4.0 statistics: [('average_q', 0.20789277831464426), ('average_loss', 0.001161001872308798)]\n",
      "episode: 120 R: 0.0 statistics: [('average_q', 0.19270985094365536), ('average_loss', 0.0008925733873637474)]\n",
      "episode: 130 R: 0.0 statistics: [('average_q', 0.17591146867298682), ('average_loss', 0.0010421552231379503)]\n",
      "episode: 140 R: 0.0 statistics: [('average_q', 0.17470657616852758), ('average_loss', 0.0011711017414970507)]\n",
      "episode: 150 R: 0.0 statistics: [('average_q', 0.16846555952387526), ('average_loss', 0.000990419548236019)]\n",
      "episode: 160 R: 0.0 statistics: [('average_q', 0.16632430195006437), ('average_loss', 0.0006307443719950892)]\n",
      "episode: 170 R: 0.0 statistics: [('average_q', 0.177362065642005), ('average_loss', 0.0011093230062058479)]\n",
      "episode: 180 R: 0.0 statistics: [('average_q', 0.1571563611007786), ('average_loss', 0.0006242434988255927)]\n",
      "episode: 190 R: 0.0 statistics: [('average_q', 0.17456315609941062), ('average_loss', 0.0008720036482868144)]\n",
      "episode: 200 R: 0.0 statistics: [('average_q', 0.15110435104409942), ('average_loss', 0.0008518269540237214)]\n",
      "episode: 210 R: 17.0 statistics: [('average_q', 0.20377273161464318), ('average_loss', 0.0004324786903215229)]\n",
      "episode: 220 R: 0.0 statistics: [('average_q', 0.20224078439058177), ('average_loss', 0.0018738034618913783)]\n",
      "episode: 230 R: 0.0 statistics: [('average_q', 0.18652254569918486), ('average_loss', 0.001692408318258888)]\n",
      "episode: 240 R: 0.0 statistics: [('average_q', 0.16106295122867664), ('average_loss', 0.0015355982276640755)]\n",
      "episode: 250 R: 0.0 statistics: [('average_q', 0.16474782290321016), ('average_loss', 0.0022441128002646223)]\n",
      "episode: 260 R: 0.0 statistics: [('average_q', 0.12355306460076959), ('average_loss', 0.0017159604517362275)]\n",
      "episode: 270 R: 0.0 statistics: [('average_q', 0.11675852332731684), ('average_loss', 0.002050546557617564)]\n",
      "episode: 280 R: 0.0 statistics: [('average_q', 0.15522624947617703), ('average_loss', 0.0019997686171782827)]\n",
      "episode: 290 R: 0.0 statistics: [('average_q', 0.21536391585409034), ('average_loss', 0.002093094270005078)]\n",
      "episode: 300 R: 0.0 statistics: [('average_q', 0.1551440079914283), ('average_loss', 0.0016562640410787708)]\n",
      "episode: 310 R: 4.0 statistics: [('average_q', 0.13641978132922436), ('average_loss', 0.001949104105803245)]\n",
      "episode: 320 R: 0.0 statistics: [('average_q', 0.38449079320193597), ('average_loss', 0.0022103633506393226)]\n",
      "episode: 330 R: 0.0 statistics: [('average_q', 0.34252340455835806), ('average_loss', 0.002582640103421033)]\n",
      "episode: 340 R: 0.0 statistics: [('average_q', 0.4928188978400732), ('average_loss', 0.0033397040869449636)]\n",
      "episode: 350 R: 0.0 statistics: [('average_q', 0.4196906833637509), ('average_loss', 0.0028508086103730007)]\n",
      "episode: 360 R: 0.0 statistics: [('average_q', 0.27340757237042523), ('average_loss', 0.002546506647518891)]\n",
      "episode: 370 R: 4.0 statistics: [('average_q', 0.44711872414342685), ('average_loss', 0.0027023274914280645)]\n",
      "episode: 380 R: 0.0 statistics: [('average_q', 0.5434886184260351), ('average_loss', 0.0028890092511116307)]\n",
      "episode: 390 R: 0.0 statistics: [('average_q', 0.3908722966254982), ('average_loss', 0.003746912821347234)]\n",
      "episode: 400 R: 0.0 statistics: [('average_q', 0.44053908763623895), ('average_loss', 0.005932853659776749)]\n",
      "episode: 410 R: 9.0 statistics: [('average_q', 0.7580041890041439), ('average_loss', 0.005155211877089283)]\n",
      "episode: 420 R: 13.0 statistics: [('average_q', 0.8087401400232921), ('average_loss', 0.005777634452220382)]\n",
      "episode: 430 R: 0.0 statistics: [('average_q', 0.7755413698076907), ('average_loss', 0.003906262826335243)]\n",
      "episode: 440 R: 0.0 statistics: [('average_q', 0.7454999220998415), ('average_loss', 0.004293845411968358)]\n",
      "episode: 450 R: 0.0 statistics: [('average_q', 0.6063200819853621), ('average_loss', 0.004168536918784713)]\n",
      "episode: 460 R: 23.0 statistics: [('average_q', 0.6840160823314811), ('average_loss', 0.004427437915089101)]\n",
      "episode: 470 R: 3.0 statistics: [('average_q', 0.6286714855532796), ('average_loss', 0.005060232095695439)]\n",
      "episode: 480 R: 0.0 statistics: [('average_q', 0.5308874774512532), ('average_loss', 0.005770354381390387)]\n",
      "episode: 490 R: 0.0 statistics: [('average_q', 0.8178352385234592), ('average_loss', 0.006553959425211431)]\n",
      "episode: 500 R: 4.0 statistics: [('average_q', 0.7878306408798591), ('average_loss', 0.007035115804517953)]\n",
      "episode: 510 R: 0.0 statistics: [('average_q', 1.6825651485858928), ('average_loss', 0.00865224163837146)]\n",
      "episode: 520 R: 0.0 statistics: [('average_q', 1.314051601124717), ('average_loss', 0.009909929626868626)]\n",
      "episode: 530 R: 3.0 statistics: [('average_q', 1.569156272644798), ('average_loss', 0.010956154391036048)]\n",
      "episode: 540 R: 6.0 statistics: [('average_q', 2.5405835449994396), ('average_loss', 0.010826616748666123)]\n",
      "episode: 550 R: 43.0 statistics: [('average_q', 2.6370978744161757), ('average_loss', 0.014008703622581165)]\n",
      "episode: 560 R: 0.0 statistics: [('average_q', 1.3295534303298087), ('average_loss', 0.01341337068389923)]\n",
      "episode: 570 R: 0.0 statistics: [('average_q', 3.1277848026319077), ('average_loss', 0.01748705381449671)]\n",
      "episode: 580 R: 13.0 statistics: [('average_q', 2.6926159893269546), ('average_loss', 0.0192456438246428)]\n",
      "episode: 590 R: 0.0 statistics: [('average_q', 4.425193286364351), ('average_loss', 0.022758869416218745)]\n",
      "episode: 600 R: 0.0 statistics: [('average_q', 3.752851692805639), ('average_loss', 0.02975599156949638)]\n",
      "episode: 610 R: 20.0 statistics: [('average_q', 3.4643976632763445), ('average_loss', 0.025429231973687624)]\n",
      "episode: 620 R: 24.0 statistics: [('average_q', 4.44210185895534), ('average_loss', 0.042742977434445044)]\n",
      "episode: 630 R: 5.0 statistics: [('average_q', 5.324247172868859), ('average_loss', 0.035854970175916004)]\n",
      "episode: 640 R: 68.0 statistics: [('average_q', 5.730720494885199), ('average_loss', 0.03181151004718186)]\n",
      "episode: 650 R: 0.0 statistics: [('average_q', 5.795688455409822), ('average_loss', 0.03798723652177786)]\n",
      "episode: 660 R: 27.0 statistics: [('average_q', 4.503562975081525), ('average_loss', 0.042646019758847444)]\n",
      "episode: 670 R: 17.0 statistics: [('average_q', 6.592773365632914), ('average_loss', 0.042402072553237635)]\n",
      "episode: 680 R: 84.0 statistics: [('average_q', 6.491368368743768), ('average_loss', 0.03658482052803773)]\n",
      "episode: 690 R: 0.0 statistics: [('average_q', 7.42174227316959), ('average_loss', 0.039468581237018505)]\n",
      "episode: 700 R: 57.0 statistics: [('average_q', 9.049215961533731), ('average_loss', 0.04234029225280153)]\n",
      "episode: 710 R: 84.0 statistics: [('average_q', 11.310486536952013), ('average_loss', 0.04501085369155092)]\n",
      "episode: 720 R: 108.0 statistics: [('average_q', 9.412796143935852), ('average_loss', 0.0326031920615003)]\n",
      "episode: 730 R: 46.0 statistics: [('average_q', 9.6173571647176), ('average_loss', 0.06031841696284952)]\n",
      "episode: 740 R: 0.0 statistics: [('average_q', 7.17566688325778), ('average_loss', 0.04674212090751567)]\n",
      "episode: 750 R: 0.0 statistics: [('average_q', 9.413870896783957), ('average_loss', 0.034846936692176186)]\n",
      "episode: 760 R: 16.0 statistics: [('average_q', 7.260066781880096), ('average_loss', 0.03309793169549309)]\n",
      "episode: 770 R: 12.0 statistics: [('average_q', 8.667782435124485), ('average_loss', 0.06487906348372761)]\n",
      "episode: 780 R: 40.0 statistics: [('average_q', 10.429942339608548), ('average_loss', 0.044618326924898276)]\n",
      "episode: 790 R: 12.0 statistics: [('average_q', 9.41820289230406), ('average_loss', 0.03559091429003518)]\n",
      "episode: 800 R: 44.0 statistics: [('average_q', 8.456256416325687), ('average_loss', 0.029979178199153342)]\n",
      "episode: 810 R: 118.0 statistics: [('average_q', 10.064611067522279), ('average_loss', 0.046614992164450496)]\n",
      "episode: 820 R: 0.0 statistics: [('average_q', 10.551318741070268), ('average_loss', 0.03635158251932505)]\n",
      "episode: 830 R: 90.0 statistics: [('average_q', 10.63525962215563), ('average_loss', 0.04431780315492244)]\n",
      "episode: 840 R: 111.0 statistics: [('average_q', 11.466746690873393), ('average_loss', 0.04565271528221483)]\n",
      "episode: 850 R: 110.0 statistics: [('average_q', 11.522232754306547), ('average_loss', 0.060814531258381294)]\n",
      "episode: 860 R: 34.0 statistics: [('average_q', 8.856844634270617), ('average_loss', 0.03742967513763826)]\n",
      "episode: 870 R: 74.0 statistics: [('average_q', 10.784306371376841), ('average_loss', 0.03507322084648378)]\n",
      "episode: 880 R: 34.0 statistics: [('average_q', 10.607604902949237), ('average_loss', 0.038705080980042175)]\n",
      "episode: 890 R: 8.0 statistics: [('average_q', 9.406170031955996), ('average_loss', 0.03170638698977727)]\n",
      "episode: 900 R: 24.0 statistics: [('average_q', 8.826185261572528), ('average_loss', 0.03703661932083699)]\n",
      "episode: 910 R: 43.0 statistics: [('average_q', 8.512013963775523), ('average_loss', 0.03196827137503437)]\n",
      "episode: 920 R: 10.0 statistics: [('average_q', 9.687590012019404), ('average_loss', 0.036175260801132)]\n",
      "episode: 930 R: 67.0 statistics: [('average_q', 11.264552969957412), ('average_loss', 0.04823488290336713)]\n",
      "episode: 940 R: 36.0 statistics: [('average_q', 11.046616417012107), ('average_loss', 0.026385531470008147)]\n",
      "episode: 950 R: 41.0 statistics: [('average_q', 10.245830872973572), ('average_loss', 0.03809045722332377)]\n",
      "episode: 960 R: 24.0 statistics: [('average_q', 9.733203917110233), ('average_loss', 0.039231808399709044)]\n",
      "episode: 970 R: 54.0 statistics: [('average_q', 10.476686513178473), ('average_loss', 0.033339903655346836)]\n",
      "episode: 980 R: 103.0 statistics: [('average_q', 11.649505464481647), ('average_loss', 0.04569461197155389)]\n",
      "episode: 990 R: 45.0 statistics: [('average_q', 10.367709992848367), ('average_loss', 0.06323504010715528)]\n",
      "episode: 1000 R: 42.0 statistics: [('average_q', 10.793975530441315), ('average_loss', 0.05532498363078074)]\n",
      "episode: 1010 R: 22.0 statistics: [('average_q', 10.670472745724028), ('average_loss', 0.045038537329982684)]\n",
      "episode: 1020 R: 49.0 statistics: [('average_q', 11.186969167282681), ('average_loss', 0.05296810549006892)]\n",
      "episode: 1030 R: 93.0 statistics: [('average_q', 9.830340424063287), ('average_loss', 0.04251786181002743)]\n",
      "episode: 1040 R: 70.0 statistics: [('average_q', 10.788915524474604), ('average_loss', 0.046171351181114456)]\n",
      "episode: 1050 R: 61.0 statistics: [('average_q', 10.716081996980769), ('average_loss', 0.05479150183451935)]\n",
      "episode: 1060 R: 81.0 statistics: [('average_q', 10.696270725001115), ('average_loss', 0.0515465817283432)]\n",
      "episode: 1070 R: 74.0 statistics: [('average_q', 10.590963914678484), ('average_loss', 0.04028306670684685)]\n",
      "episode: 1080 R: 8.0 statistics: [('average_q', 10.444922860009857), ('average_loss', 0.04779104456599524)]\n",
      "episode: 1090 R: 74.0 statistics: [('average_q', 9.718388310607276), ('average_loss', 0.03548744410354695)]\n",
      "episode: 1100 R: 30.0 statistics: [('average_q', 10.1233882523871), ('average_loss', 0.04588152886835738)]\n",
      "episode: 1110 R: 31.0 statistics: [('average_q', 9.113983261601584), ('average_loss', 0.06154948161432947)]\n",
      "episode: 1120 R: 8.0 statistics: [('average_q', 10.285279502278248), ('average_loss', 0.049230160220398285)]\n",
      "episode: 1130 R: 45.0 statistics: [('average_q', 10.053128537711915), ('average_loss', 0.059298469140385675)]\n",
      "episode: 1140 R: 31.0 statistics: [('average_q', 9.857209179352704), ('average_loss', 0.030033590764954616)]\n",
      "episode: 1150 R: 0.0 statistics: [('average_q', 8.248103492198306), ('average_loss', 0.050271857184759426)]\n",
      "episode: 1160 R: 50.0 statistics: [('average_q', 10.144423545530438), ('average_loss', 0.04360578936555571)]\n",
      "episode: 1170 R: 33.0 statistics: [('average_q', 10.761133474415546), ('average_loss', 0.04386978240248951)]\n",
      "episode: 1180 R: 70.0 statistics: [('average_q', 9.770338676227958), ('average_loss', 0.04476252053382463)]\n",
      "episode: 1190 R: 97.0 statistics: [('average_q', 10.895579610895908), ('average_loss', 0.06547648595737739)]\n",
      "episode: 1200 R: 10.0 statistics: [('average_q', 10.29334939398588), ('average_loss', 0.048366231621432026)]\n",
      "episode: 1210 R: 46.0 statistics: [('average_q', 10.594625227824801), ('average_loss', 0.04341736292086298)]\n",
      "episode: 1220 R: 11.0 statistics: [('average_q', 9.853616640541095), ('average_loss', 0.04391834020865963)]\n",
      "episode: 1230 R: 53.0 statistics: [('average_q', 11.225274068687304), ('average_loss', 0.05488382507524055)]\n",
      "episode: 1240 R: 36.0 statistics: [('average_q', 10.320886209518084), ('average_loss', 0.05490016266066819)]\n",
      "episode: 1250 R: 118.0 statistics: [('average_q', 10.351640815872619), ('average_loss', 0.05222036321363456)]\n",
      "episode: 1260 R: 75.0 statistics: [('average_q', 8.927187640496399), ('average_loss', 0.061900177108790234)]\n",
      "episode: 1270 R: 121.0 statistics: [('average_q', 11.031713993735616), ('average_loss', 0.04026127807302199)]\n",
      "episode: 1280 R: 3.0 statistics: [('average_q', 10.402640516248018), ('average_loss', 0.04454292128655525)]\n",
      "episode: 1290 R: 60.0 statistics: [('average_q', 10.584610340415828), ('average_loss', 0.05413636372951269)]\n",
      "episode: 1300 R: 0.0 statistics: [('average_q', 8.646885252814151), ('average_loss', 0.04963127378410729)]\n",
      "episode: 1310 R: 115.0 statistics: [('average_q', 11.53753398215044), ('average_loss', 0.04540421735209155)]\n",
      "episode: 1320 R: 68.0 statistics: [('average_q', 8.926376801672754), ('average_loss', 0.04023940444684958)]\n",
      "episode: 1330 R: 120.0 statistics: [('average_q', 10.342044837521465), ('average_loss', 0.035729380017481605)]\n",
      "episode: 1340 R: 88.0 statistics: [('average_q', 10.473323264255345), ('average_loss', 0.0397287506329317)]\n",
      "episode: 1350 R: 74.0 statistics: [('average_q', 10.096072147192109), ('average_loss', 0.053250437459399884)]\n",
      "episode: 1360 R: 107.0 statistics: [('average_q', 10.848897699051413), ('average_loss', 0.05292431169728074)]\n",
      "episode: 1370 R: 100.0 statistics: [('average_q', 10.28159176901401), ('average_loss', 0.03077884770063555)]\n",
      "episode: 1380 R: 60.0 statistics: [('average_q', 10.95347889374319), ('average_loss', 0.03871604325521711)]\n",
      "episode: 1390 R: 60.0 statistics: [('average_q', 9.54736044417319), ('average_loss', 0.03940370804167552)]\n",
      "episode: 1400 R: 102.0 statistics: [('average_q', 9.725053399412827), ('average_loss', 0.041839245121836774)]\n",
      "episode: 1410 R: 9.0 statistics: [('average_q', 10.568408958793835), ('average_loss', 0.05078228861635963)]\n",
      "episode: 1420 R: 116.0 statistics: [('average_q', 12.233244573906797), ('average_loss', 0.04414919530700041)]\n",
      "episode: 1430 R: 104.0 statistics: [('average_q', 9.65547067944744), ('average_loss', 0.04276285419579045)]\n",
      "episode: 1440 R: 8.0 statistics: [('average_q', 9.899454219800921), ('average_loss', 0.03444479570926602)]\n",
      "episode: 1450 R: 94.0 statistics: [('average_q', 11.779279901736814), ('average_loss', 0.053734794920779524)]\n",
      "episode: 1460 R: 28.0 statistics: [('average_q', 9.64579432529509), ('average_loss', 0.037239003106715664)]\n",
      "episode: 1470 R: 63.0 statistics: [('average_q', 10.83038062460253), ('average_loss', 0.04614816084378411)]\n",
      "episode: 1480 R: 4.0 statistics: [('average_q', 10.058924511072975), ('average_loss', 0.05055943395346709)]\n",
      "episode: 1490 R: 40.0 statistics: [('average_q', 10.056505595002417), ('average_loss', 0.032771191701485736)]\n",
      "episode: 1500 R: 11.0 statistics: [('average_q', 9.588359088260518), ('average_loss', 0.030553264375916444)]\n",
      "episode: 1510 R: 75.0 statistics: [('average_q', 10.31092592666336), ('average_loss', 0.033320586662925714)]\n",
      "episode: 1520 R: 94.0 statistics: [('average_q', 11.362652803724796), ('average_loss', 0.045679689604828634)]\n",
      "episode: 1530 R: 8.0 statistics: [('average_q', 10.517745860063775), ('average_loss', 0.030012575114076758)]\n",
      "episode: 1540 R: 99.0 statistics: [('average_q', 11.867951092947811), ('average_loss', 0.03070001147249268)]\n",
      "episode: 1550 R: 0.0 statistics: [('average_q', 10.469718282735828), ('average_loss', 0.044587278786113994)]\n",
      "episode: 1560 R: 4.0 statistics: [('average_q', 9.643529307396562), ('average_loss', 0.02768026668474443)]\n",
      "episode: 1570 R: 97.0 statistics: [('average_q', 9.935848031238812), ('average_loss', 0.03579979523821576)]\n",
      "episode: 1580 R: 64.0 statistics: [('average_q', 10.772325647172837), ('average_loss', 0.04344628649102885)]\n",
      "episode: 1590 R: 100.0 statistics: [('average_q', 10.83370773821085), ('average_loss', 0.03763741758516833)]\n",
      "episode: 1600 R: 54.0 statistics: [('average_q', 11.082325862072418), ('average_loss', 0.04727896124758724)]\n",
      "episode: 1610 R: 119.0 statistics: [('average_q', 11.928995744097605), ('average_loss', 0.03446941918352365)]\n",
      "episode: 1620 R: 82.0 statistics: [('average_q', 11.599911816882845), ('average_loss', 0.03776874783496809)]\n",
      "episode: 1630 R: 86.0 statistics: [('average_q', 11.905357753772828), ('average_loss', 0.044986514137068)]\n",
      "episode: 1640 R: 110.0 statistics: [('average_q', 12.55447572476799), ('average_loss', 0.03078944589574201)]\n",
      "episode: 1650 R: 120.0 statistics: [('average_q', 12.88370419536156), ('average_loss', 0.03546633452742298)]\n",
      "episode: 1660 R: 22.0 statistics: [('average_q', 10.687801570941698), ('average_loss', 0.032324682753168736)]\n",
      "episode: 1670 R: 40.0 statistics: [('average_q', 10.056388626330238), ('average_loss', 0.040873883111005115)]\n",
      "episode: 1680 R: 116.0 statistics: [('average_q', 9.652020492884233), ('average_loss', 0.033965894617102727)]\n",
      "episode: 1690 R: 69.0 statistics: [('average_q', 11.122118064221995), ('average_loss', 0.032658709293877056)]\n",
      "episode: 1700 R: 45.0 statistics: [('average_q', 9.66630853726203), ('average_loss', 0.02980088117505443)]\n",
      "episode: 1710 R: 0.0 statistics: [('average_q', 10.196791013243182), ('average_loss', 0.04768252075767203)]\n",
      "episode: 1720 R: 73.0 statistics: [('average_q', 12.459330213996497), ('average_loss', 0.03974694579033385)]\n",
      "episode: 1730 R: 116.0 statistics: [('average_q', 11.325161219301783), ('average_loss', 0.028591204080716515)]\n",
      "episode: 1740 R: 70.0 statistics: [('average_q', 10.026332561453394), ('average_loss', 0.028946236055342495)]\n",
      "episode: 1750 R: 37.0 statistics: [('average_q', 7.912187877440102), ('average_loss', 0.037147759500491685)]\n",
      "episode: 1760 R: 21.0 statistics: [('average_q', 10.228079849807132), ('average_loss', 0.05062250422434607)]\n",
      "episode: 1770 R: 114.0 statistics: [('average_q', 10.989709601572208), ('average_loss', 0.023972749981016228)]\n",
      "episode: 1780 R: 36.0 statistics: [('average_q', 9.577893635613922), ('average_loss', 0.03232689313797542)]\n",
      "episode: 1790 R: 46.0 statistics: [('average_q', 10.845410586294939), ('average_loss', 0.031249382413697464)]\n",
      "episode: 1800 R: 107.0 statistics: [('average_q', 11.030647506779161), ('average_loss', 0.02851227644728352)]\n",
      "episode: 1810 R: 45.0 statistics: [('average_q', 10.41354070767527), ('average_loss', 0.02905704788884512)]\n",
      "episode: 1820 R: 115.0 statistics: [('average_q', 11.080752379404226), ('average_loss', 0.028938886986223395)]\n",
      "episode: 1830 R: 69.0 statistics: [('average_q', 9.533482850076796), ('average_loss', 0.031199773821535638)]\n",
      "episode: 1840 R: 6.0 statistics: [('average_q', 9.576834518188573), ('average_loss', 0.033164648895111566)]\n",
      "episode: 1850 R: 98.0 statistics: [('average_q', 10.85750597901702), ('average_loss', 0.03129443812460478)]\n",
      "episode: 1860 R: 108.0 statistics: [('average_q', 10.683586444477276), ('average_loss', 0.026291987584518194)]\n",
      "episode: 1870 R: 9.0 statistics: [('average_q', 10.023535638868418), ('average_loss', 0.03504371161293996)]\n",
      "episode: 1880 R: 104.0 statistics: [('average_q', 10.585735651432978), ('average_loss', 0.023330999718881463)]\n",
      "episode: 1890 R: 30.0 statistics: [('average_q', 10.080245886934398), ('average_loss', 0.027056945220876896)]\n",
      "episode: 1900 R: 52.0 statistics: [('average_q', 9.909036218508142), ('average_loss', 0.03296462504622254)]\n",
      "episode: 1910 R: 99.0 statistics: [('average_q', 10.867051482568298), ('average_loss', 0.03269372604112693)]\n",
      "episode: 1920 R: 35.0 statistics: [('average_q', 9.795286045078104), ('average_loss', 0.035192123270227166)]\n",
      "episode: 1930 R: 16.0 statistics: [('average_q', 8.639980171774011), ('average_loss', 0.029766227205217793)]\n",
      "episode: 1940 R: 107.0 statistics: [('average_q', 8.921905926025111), ('average_loss', 0.036006870225004164)]\n",
      "episode: 1950 R: 74.0 statistics: [('average_q', 10.374395618791278), ('average_loss', 0.03218452608757027)]\n",
      "episode: 1960 R: 45.0 statistics: [('average_q', 10.992797379180397), ('average_loss', 0.027588781805691673)]\n",
      "episode: 1970 R: 102.0 statistics: [('average_q', 10.207970963773233), ('average_loss', 0.027178245364781396)]\n",
      "episode: 1980 R: 18.0 statistics: [('average_q', 8.46622032286377), ('average_loss', 0.038082337190485434)]\n",
      "episode: 1990 R: 63.0 statistics: [('average_q', 9.18305581957926), ('average_loss', 0.025713566080892224)]\n",
      "episode: 2000 R: 108.0 statistics: [('average_q', 10.818912250227056), ('average_loss', 0.0368943569715634)]\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 2000\n",
    "max_episode_len = 200\n",
    "for i in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    reward = 0\n",
    "    done = False\n",
    "    R = 0  # return (sum of rewards)\n",
    "    t = 0  # time step\n",
    "    while not done and t < max_episode_len:\n",
    "        # Uncomment to watch the behaviour\n",
    "        # env.render()\n",
    "        action = agent.act_and_train(obs, reward)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        t += 1\n",
    "    if i % 10 == 0:\n",
    "        print('episode:', i,\n",
    "              'R:', R,\n",
    "              'statistics:', agent.get_statistics())\n",
    "    agent.stop_episode_and_train(obs, reward, done)\n",
    "print('Finished.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test episode: 0 R: 62.0\n",
      "test episode: 1 R: 67.0\n",
      "test episode: 2 R: 64.0\n",
      "test episode: 3 R: 62.0\n",
      "test episode: 4 R: 61.0\n",
      "test episode: 5 R: 63.0\n",
      "test episode: 6 R: 108.0\n",
      "test episode: 7 R: 63.0\n",
      "test episode: 8 R: 64.0\n",
      "test episode: 9 R: 63.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    R = 0\n",
    "    t = 0\n",
    "    while not done and t < 200:\n",
    "        # env.render()\n",
    "        action = agent.act(obs)\n",
    "        obs, r, done, _ = env.step(action)\n",
    "        R += r\n",
    "        t += 1\n",
    "    print('test episode:', i, 'R:', R)\n",
    "    agent.stop_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
